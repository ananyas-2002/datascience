{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "81fc9749",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\user\\anaconda3\\lib\\site-packages (2.2.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from pandas) (2021.3)\n",
      "Requirement already satisfied: numpy>=1.22.4 in c:\\users\\user\\anaconda3\\lib\\site-packages (from pandas) (1.22.4)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\user\\anaconda3\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\user\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -umpy (c:\\users\\user\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\user\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\users\\user\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\user\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\users\\user\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\user\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\users\\user\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\user\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\users\\user\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\user\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\users\\user\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\user\\anaconda3\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "7ae3b65d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "7b27fc7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv('Training.tsv',sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "ee1ffcb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1382343793341575169</td>\n",
       "      <td>@IrvineWelsh   I don’t know about you Irvine b...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1377631738692796417</td>\n",
       "      <td>https://t.co/T2aD3v6dpm I bet money if i went...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1386448010029240326</td>\n",
       "      <td>@JamesMelville My wife received a POSITIVE Cov...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1361342676340211717</td>\n",
       "      <td>Out of the 180,000+ people who have had the tw...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1386757983254765569</td>\n",
       "      <td>My whole family is sick af and here I am now i...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              tweet_id                                               text  \\\n",
       "0  1382343793341575169  @IrvineWelsh   I don’t know about you Irvine b...   \n",
       "1  1377631738692796417   https://t.co/T2aD3v6dpm I bet money if i went...   \n",
       "2  1386448010029240326  @JamesMelville My wife received a POSITIVE Cov...   \n",
       "3  1361342676340211717  Out of the 180,000+ people who have had the tw...   \n",
       "4  1386757983254765569  My whole family is sick af and here I am now i...   \n",
       "\n",
       "   label  \n",
       "0      0  \n",
       "1      0  \n",
       "2      0  \n",
       "3      0  \n",
       "4      0  "
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "469c1426",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  define your own list of stopwords\n",
    "english_stopwords = set([\n",
    "        \"i\", \"me\", \"my\", \"myself\", \"we\", \"our\", \"ours\", \"ourselves\", \"you\", \"your\", \n",
    "        \"yours\", \"yourself\", \"yourselves\", \"he\", \"him\", \"his\", \"himself\", \"she\", \n",
    "        \"her\", \"hers\", \"herself\", \"it\", \"its\", \"itself\", \"they\", \"them\", \"their\", \n",
    "        \"theirs\", \"themselves\", \"what\", \"which\", \"who\", \"whom\", \"this\", \"that\", \n",
    "        \"these\", \"those\", \"am\", \"is\", \"are\", \"was\", \"were\", \"be\", \"been\", \"being\", \n",
    "        \"have\", \"has\", \"had\", \"having\", \"do\", \"does\", \"did\", \"doing\", \"a\", \"an\", \n",
    "        \"the\", \"and\", \"but\", \"if\", \"or\", \"because\", \"as\", \"until\", \"while\", \"of\", \n",
    "        \"at\", \"by\", \"for\", \"with\", \"about\", \"against\", \"between\", \"into\", \"through\", \n",
    "        \"during\", \"before\", \"after\", \"above\", \"below\", \"to\", \"from\", \"up\", \"down\", \n",
    "        \"in\", \"out\", \"on\", \"off\", \"over\", \"under\", \"again\", \"further\", \"then\", \"once\"\n",
    "    ])\n",
    "def preprocess(text):\n",
    "     #lower case\n",
    "    text=to_lower_case(text)\n",
    "    \n",
    "    #remove contraction\n",
    "    text = text.replace(\"’\", \"'\") #replace curly aposrophe with straight aposrophe\n",
    "    text=remove_contraction(text)\n",
    "    \n",
    "    # Replace digits with words\n",
    "    text = replace_digits_with_words(text)\n",
    "    \n",
    "     #remove url's\n",
    "    text=remove_urls(text)\n",
    "    \n",
    "    # Remove special characters (punctuation)\n",
    "    text = remove_special_characters(text)\n",
    "    \n",
    "    # Tokenize the text manually (splitting by whitespace)\n",
    "    tokens = text.split()\n",
    "    tokens = [token for token in tokens if token not in english_stopwords]\n",
    "    \n",
    "    # Join tokens back into a string\n",
    "    processed_text = \" \".join(tokens)\n",
    "    \n",
    "    return processed_text\n",
    "def remove_contraction(text):\n",
    "    # Expand contractions manually (you can add more as needed)\n",
    "    contractions_map = {\n",
    "        \"isn't\": \"is not\",\n",
    "        \"aren't\": \"are not\",\n",
    "        \"wasn't\": \"was not\",\n",
    "        \"weren't\": \"were not\",\n",
    "        \"haven't\": \"have not\",\n",
    "        \"hasn't\": \"has not\",\n",
    "        \"hadn't\": \"had not\",\n",
    "        \"won't\": \"will not\",\n",
    "        \"wouldn't\": \"would not\",\n",
    "        \"don't\": \"do not\",\n",
    "        \"doesn't\": \"does not\",\n",
    "        \"didn't\": \"did not\",\n",
    "        \"can't\": \"cannot\",\n",
    "        \"couldn't\": \"could not\",\n",
    "        \"shouldn't\": \"should not\",\n",
    "        \"mightn't\": \"might not\",\n",
    "        \"mustn't\": \"must not\",\n",
    "        \"'s\": \" is\",\n",
    "        \"'re\": \" are\",\n",
    "        \"'m\": \" am\",\n",
    "        \"'ll\": \" will\"\n",
    "    }\n",
    "    for contraction, expansion in contractions_map.items():\n",
    "        text = text.replace(contraction,expansion)\n",
    "    return text\n",
    "    \n",
    "def replace_digits_with_words(text):\n",
    "    # Map digits to their word representations (0-9 for simplicity)\n",
    "    digit_map = {\n",
    "        '0': 'zero', '1': 'one', '2': 'two', '3': 'three', '4': 'four',\n",
    "        '5': 'five', '6': 'six', '7': 'seven', '8': 'eight', '9': 'nine'\n",
    "    }\n",
    "    \n",
    "    # Replace digits with their word representations\n",
    "    for digit, word in digit_map.items():\n",
    "        text = text.replace(digit, word)\n",
    "    \n",
    "    return text\n",
    "\n",
    "def remove_special_characters(text):\n",
    "    # Define a set of special characters to remove\n",
    "    special_characters = set(\"!\\\"#$%&'()*+,-./:;<=>?@[\\\\]^_`{|}~\")\n",
    "    \n",
    "    # Remove special characters from the text\n",
    "    cleaned_text = \"\".join(char for char in text if char not in special_characters)\n",
    "    \n",
    "    return cleaned_text\n",
    "def to_lower_case(text):\n",
    "    lower_text = \"\"\n",
    "    for char in text:\n",
    "        # Check if character is uppercase\n",
    "        if 'A' <= char <= 'Z':\n",
    "            # Convert uppercase character to lowercase\n",
    "            lower_text += chr(ord(char) + 32)\n",
    "        else:\n",
    "            lower_text += char\n",
    "    return lower_text\n",
    "def remove_urls(text):\n",
    "    # Split text into words\n",
    "    words = text.split()\n",
    "    # Filter out words that do not start with 'http' or 'https'\n",
    "    filtered_words = [word for word in words if not (word.startswith('http://') or word.startswith('https://'))]\n",
    "    # Join the filtered words back into a string\n",
    "    return ' '.join(filtered_words)\n",
    "\n",
    "\n",
    "# Load your DataFrame from 'training.tsv' assuming it contains text data\n",
    "df = pd.read_csv('training.tsv', sep='\\t')\n",
    "\n",
    "# Apply the preprocess function to each element in the DataFrame\n",
    "df['cleaned_text'] = df['text'].apply(preprocess)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "f95c3d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import emoji\n",
    "#import demoji\n",
    "\n",
    "#demoji.download_codes()\n",
    "def emo(text):\n",
    "  temp=emoji.demojize(text,delimiters=(\" \",\" \"))\n",
    "  temp=temp.replace(\"_\",\"  \")\n",
    "  return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "9caace11",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['emo']=data[\"text\"].apply(lambda x:emo(x))\n",
    "data[\"clean_text\"]=data['emo'].apply(lambda X: preprocess(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "a779c267",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>emo</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1382343793341575169</td>\n",
       "      <td>@IrvineWelsh   I don’t know about you Irvine b...</td>\n",
       "      <td>0</td>\n",
       "      <td>@IrvineWelsh   I don’t know about you Irvine b...</td>\n",
       "      <td>irvinewelsh not know irvine keep told covid no...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1377631738692796417</td>\n",
       "      <td>https://t.co/T2aD3v6dpm I bet money if i went...</td>\n",
       "      <td>0</td>\n",
       "      <td>https://t.co/T2aD3v6dpm I bet money if i went...</td>\n",
       "      <td>bet money went n took covid test right now imm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1386448010029240326</td>\n",
       "      <td>@JamesMelville My wife received a POSITIVE Cov...</td>\n",
       "      <td>0</td>\n",
       "      <td>@JamesMelville My wife received a POSITIVE Cov...</td>\n",
       "      <td>jamesmelville wife received positive covid tes...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1361342676340211717</td>\n",
       "      <td>Out of the 180,000+ people who have had the tw...</td>\n",
       "      <td>0</td>\n",
       "      <td>Out of the 180,000+ people who have had the tw...</td>\n",
       "      <td>oneeightzerozerozerozero people two vaccine sh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1386757983254765569</td>\n",
       "      <td>My whole family is sick af and here I am now i...</td>\n",
       "      <td>0</td>\n",
       "      <td>My whole family is sick af and here I am now i...</td>\n",
       "      <td>whole family sick af here now hospital heart p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1382001700853125122</td>\n",
       "      <td>@renfrew1962 @PeakePolly @J_Deliciouso I'm not...</td>\n",
       "      <td>0</td>\n",
       "      <td>@renfrew1962 @PeakePolly @J  Deliciouso I'm no...</td>\n",
       "      <td>renfrewoneninesixtwo peakepolly j deliciouso n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1383272654212272136</td>\n",
       "      <td>Test came back positive, no surprise. I have c...</td>\n",
       "      <td>1</td>\n",
       "      <td>Test came back positive, no surprise. I have c...</td>\n",
       "      <td>test came back positive no surprise covid full...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1374479299047084035</td>\n",
       "      <td>My Pawpaw has been in the hospital a few days....</td>\n",
       "      <td>0</td>\n",
       "      <td>My Pawpaw has been in the hospital a few days....</td>\n",
       "      <td>pawpaw hospital few days got special approval ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1354020426620547072</td>\n",
       "      <td>@MattHancock 4 people I know had covid and rec...</td>\n",
       "      <td>0</td>\n",
       "      <td>@MattHancock 4 people I know had covid and rec...</td>\n",
       "      <td>matthancock four people know covid recovered e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1362671045136809985</td>\n",
       "      <td>I’m going to sound like I have lost my marbles...</td>\n",
       "      <td>1</td>\n",
       "      <td>I’m going to sound like I have lost my marbles...</td>\n",
       "      <td>going sound like lost marbles not felt well si...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1373077971658022918</td>\n",
       "      <td>I just tested positive for Covid-19 from two s...</td>\n",
       "      <td>1</td>\n",
       "      <td>I just tested positive for Covid-19 from two s...</td>\n",
       "      <td>just tested positive covidonenine two separate...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1373076070312730633</td>\n",
       "      <td>Someone I love very much was diagnosed with co...</td>\n",
       "      <td>0</td>\n",
       "      <td>Someone I love very much was diagnosed with co...</td>\n",
       "      <td>someone love very much diagnosed covid onenine...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1331292744489250816</td>\n",
       "      <td>Dear @PeterHotez ,Serious question about Vacci...</td>\n",
       "      <td>0</td>\n",
       "      <td>Dear @PeterHotez ,Serious question about Vacci...</td>\n",
       "      <td>dear peterhotez serious question vaccinein one...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1379116535726415874</td>\n",
       "      <td>As I was in the ER last night I overheard a co...</td>\n",
       "      <td>1</td>\n",
       "      <td>As I was in the ER last night I overheard a co...</td>\n",
       "      <td>er last night overheard conversation someone w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1357823724192296960</td>\n",
       "      <td>@jrlsilverman @dilleradollar @OregonGovBrown 💡...</td>\n",
       "      <td>0</td>\n",
       "      <td>@jrlsilverman @dilleradollar @OregonGovBrown  ...</td>\n",
       "      <td>jrlsilverman dilleradollar oregongovbrown ligh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1376702269991890945</td>\n",
       "      <td>Today: 1 person (after overnight code) in ICU ...</td>\n",
       "      <td>0</td>\n",
       "      <td>Today: 1 person (after overnight code) in ICU ...</td>\n",
       "      <td>today one person overnight code icu wcovid fiv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1377607714524831745</td>\n",
       "      <td>@Asilverlining20 @TVpsychologist @NHSuk It’s b...</td>\n",
       "      <td>0</td>\n",
       "      <td>@Asilverlining20 @TVpsychologist @NHSuk It’s b...</td>\n",
       "      <td>asilverliningtwozero tvpsychologist nhsuk year...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1388377556097904640</td>\n",
       "      <td>These side effects are CRAZY!!! Got the second...</td>\n",
       "      <td>0</td>\n",
       "      <td>These side effects are CRAZY!!! Got the second...</td>\n",
       "      <td>side effects crazy got second pfizer vaccine t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1377509820425723912</td>\n",
       "      <td>@BethMooreLPM Continuing to #Pray for Healing ...</td>\n",
       "      <td>0</td>\n",
       "      <td>@BethMooreLPM Continuing to #Pray for Healing ...</td>\n",
       "      <td>bethmoorelpm continuing pray healing miracles ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1383382420234280964</td>\n",
       "      <td>@mstranack @GillianMcKeith Just spoke to Hayde...</td>\n",
       "      <td>0</td>\n",
       "      <td>@mstranack @GillianMcKeith Just spoke to Hayde...</td>\n",
       "      <td>mstranack gillianmckeith just spoke hayden mor...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               tweet_id                                               text  \\\n",
       "0   1382343793341575169  @IrvineWelsh   I don’t know about you Irvine b...   \n",
       "1   1377631738692796417   https://t.co/T2aD3v6dpm I bet money if i went...   \n",
       "2   1386448010029240326  @JamesMelville My wife received a POSITIVE Cov...   \n",
       "3   1361342676340211717  Out of the 180,000+ people who have had the tw...   \n",
       "4   1386757983254765569  My whole family is sick af and here I am now i...   \n",
       "5   1382001700853125122  @renfrew1962 @PeakePolly @J_Deliciouso I'm not...   \n",
       "6   1383272654212272136  Test came back positive, no surprise. I have c...   \n",
       "7   1374479299047084035  My Pawpaw has been in the hospital a few days....   \n",
       "8   1354020426620547072  @MattHancock 4 people I know had covid and rec...   \n",
       "9   1362671045136809985  I’m going to sound like I have lost my marbles...   \n",
       "10  1373077971658022918  I just tested positive for Covid-19 from two s...   \n",
       "11  1373076070312730633  Someone I love very much was diagnosed with co...   \n",
       "12  1331292744489250816  Dear @PeterHotez ,Serious question about Vacci...   \n",
       "13  1379116535726415874  As I was in the ER last night I overheard a co...   \n",
       "14  1357823724192296960  @jrlsilverman @dilleradollar @OregonGovBrown 💡...   \n",
       "15  1376702269991890945  Today: 1 person (after overnight code) in ICU ...   \n",
       "16  1377607714524831745  @Asilverlining20 @TVpsychologist @NHSuk It’s b...   \n",
       "17  1388377556097904640  These side effects are CRAZY!!! Got the second...   \n",
       "18  1377509820425723912  @BethMooreLPM Continuing to #Pray for Healing ...   \n",
       "19  1383382420234280964  @mstranack @GillianMcKeith Just spoke to Hayde...   \n",
       "\n",
       "    label                                                emo  \\\n",
       "0       0  @IrvineWelsh   I don’t know about you Irvine b...   \n",
       "1       0   https://t.co/T2aD3v6dpm I bet money if i went...   \n",
       "2       0  @JamesMelville My wife received a POSITIVE Cov...   \n",
       "3       0  Out of the 180,000+ people who have had the tw...   \n",
       "4       0  My whole family is sick af and here I am now i...   \n",
       "5       0  @renfrew1962 @PeakePolly @J  Deliciouso I'm no...   \n",
       "6       1  Test came back positive, no surprise. I have c...   \n",
       "7       0  My Pawpaw has been in the hospital a few days....   \n",
       "8       0  @MattHancock 4 people I know had covid and rec...   \n",
       "9       1  I’m going to sound like I have lost my marbles...   \n",
       "10      1  I just tested positive for Covid-19 from two s...   \n",
       "11      0  Someone I love very much was diagnosed with co...   \n",
       "12      0  Dear @PeterHotez ,Serious question about Vacci...   \n",
       "13      1  As I was in the ER last night I overheard a co...   \n",
       "14      0  @jrlsilverman @dilleradollar @OregonGovBrown  ...   \n",
       "15      0  Today: 1 person (after overnight code) in ICU ...   \n",
       "16      0  @Asilverlining20 @TVpsychologist @NHSuk It’s b...   \n",
       "17      0  These side effects are CRAZY!!! Got the second...   \n",
       "18      0  @BethMooreLPM Continuing to #Pray for Healing ...   \n",
       "19      0  @mstranack @GillianMcKeith Just spoke to Hayde...   \n",
       "\n",
       "                                           clean_text  \n",
       "0   irvinewelsh not know irvine keep told covid no...  \n",
       "1   bet money went n took covid test right now imm...  \n",
       "2   jamesmelville wife received positive covid tes...  \n",
       "3   oneeightzerozerozerozero people two vaccine sh...  \n",
       "4   whole family sick af here now hospital heart p...  \n",
       "5   renfrewoneninesixtwo peakepolly j deliciouso n...  \n",
       "6   test came back positive no surprise covid full...  \n",
       "7   pawpaw hospital few days got special approval ...  \n",
       "8   matthancock four people know covid recovered e...  \n",
       "9   going sound like lost marbles not felt well si...  \n",
       "10  just tested positive covidonenine two separate...  \n",
       "11  someone love very much diagnosed covid onenine...  \n",
       "12  dear peterhotez serious question vaccinein one...  \n",
       "13  er last night overheard conversation someone w...  \n",
       "14  jrlsilverman dilleradollar oregongovbrown ligh...  \n",
       "15  today one person overnight code icu wcovid fiv...  \n",
       "16  asilverliningtwozero tvpsychologist nhsuk year...  \n",
       "17  side effects crazy got second pfizer vaccine t...  \n",
       "18  bethmoorelpm continuing pray healing miracles ...  \n",
       "19  mstranack gillianmckeith just spoke hayden mor...  "
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f05b05a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "isn't : is not\n",
      "aren't : are not\n",
      "wasn't : was not\n",
      "weren't : were not\n",
      "haven't : have not\n",
      "hasn't : has not\n",
      "hadn't : had not\n",
      "won't : will not\n",
      "wouldn't : would not\n",
      "don't : do not\n",
      "doesn't : does not\n",
      "didn't : did not\n",
      "can't : cannot\n",
      "couldn't : could not\n",
      "shouldn't : should not\n",
      "mightn't : might not\n",
      "mustn't : must not\n",
      "'s :  is\n",
      "'re :  are\n",
      "'m :  am\n",
      "'ll :  will\n"
     ]
    }
   ],
   "source": [
    "contractions_map = {\n",
    "        \"isn't\": \"is not\",\n",
    "        \"aren't\": \"are not\",\n",
    "        \"wasn't\": \"was not\",\n",
    "        \"weren't\": \"were not\",\n",
    "        \"haven't\": \"have not\",\n",
    "        \"hasn't\": \"has not\",\n",
    "        \"hadn't\": \"had not\",\n",
    "        \"won't\": \"will not\",\n",
    "        \"wouldn't\": \"would not\",\n",
    "        \"don't\": \"do not\",\n",
    "        \"doesn't\": \"does not\",\n",
    "        \"didn't\": \"did not\",\n",
    "        \"can't\": \"cannot\",\n",
    "        \"couldn't\": \"could not\",\n",
    "        \"shouldn't\": \"should not\",\n",
    "        \"mightn't\": \"might not\",\n",
    "        \"mustn't\": \"must not\",\n",
    "        \"'s\": \" is\",\n",
    "        \"'re\": \" are\",\n",
    "        \"'m\": \" am\",\n",
    "        \"'ll\": \" will\"\n",
    "    }\n",
    "for contraction, expansion in contractions_map.items():\n",
    "    print(contraction,\":\",expansion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7dfa7e28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i have do not have not know\n"
     ]
    }
   ],
   "source": [
    "contractions_map = {\n",
    "        \"isn't\": \"is not\",\n",
    "        \"aren't\": \"are not\",\n",
    "        \"wasn't\": \"was not\",\n",
    "        \"weren't\": \"were not\",\n",
    "        \"haven't\": \"have not\",\n",
    "        \"hasn't\": \"has not\",\n",
    "        \"hadn't\": \"had not\",\n",
    "        \"won't\": \"will not\",\n",
    "        \"wouldn't\": \"would not\",\n",
    "        \"don't\": \"do not\",\n",
    "        \"doesn't\": \"does not\",\n",
    "        \"didn't\": \"did not\",\n",
    "        \"can't\": \"cannot\",\n",
    "        \"couldn't\": \"could not\",\n",
    "        \"shouldn't\": \"should not\",\n",
    "        \"mightn't\": \"might not\",\n",
    "        \"mustn't\": \"must not\",\n",
    "        \"'s\": \" is\",\n",
    "        \"'re\": \" are\",\n",
    "        \"'m\": \" am\",\n",
    "        \"'ll\": \" will\"\n",
    "    }\n",
    "text=\"i have don't haven't know\"\n",
    "for contraction, expansion in contractions_map.items():\n",
    "    text = text.replace(contraction,expansion)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d45f41",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c75f139b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99787bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define your preprocess function\n",
    "def preprocess(text):\n",
    "    # Expand contractions manually (you can add more as needed)\n",
    "    contractions_map = {\n",
    "        \"isn't\": \"is not\",\n",
    "        \"aren't\": \"are not\",\n",
    "        \"wasn't\": \"was not\",\n",
    "        \"weren't\": \"were not\",\n",
    "        \"haven't\": \"have not\",\n",
    "        \"hasn't\": \"has not\",\n",
    "        \"hadn't\": \"had not\",\n",
    "        \"won't\": \"will not\",\n",
    "        \"wouldn't\": \"would not\",\n",
    "        \"don't\": \"do not\",\n",
    "        \"doesn't\": \"does not\",\n",
    "        \"didn't\": \"did not\",\n",
    "        \"can't\": \"cannot\",\n",
    "        \"couldn't\": \"could not\",\n",
    "        \"shouldn't\": \"should not\",\n",
    "        \"mightn't\": \"might not\",\n",
    "        \"mustn't\": \"must not\",\n",
    "        \"'s\": \" is\",\n",
    "        \"'re\": \" are\",\n",
    "        \"'m\": \" am\",\n",
    "        \"'ll\": \" will\"\n",
    "    }\n",
    "    \n",
    "    for contraction, expansion in contractions_map.items():\n",
    "        text = text.replace(contraction,expansion)\n",
    "    #lower case\n",
    "    text=to_lower_case(text)\n",
    "        \n",
    "    # Replace digits with words\n",
    "    text = replace_digits_with_words(text)\n",
    "    \n",
    "    # Remove special characters (punctuation)\n",
    "    text = remove_special_characters(text)\n",
    "    #remove url's\n",
    "    text=remove_urls(text)\n",
    "    \n",
    "    # Tokenize the text manually (splitting by whitespace)\n",
    "    tokens = text.split()\n",
    "    \n",
    "    # Remove stopwords (you can define your own list of stopwords)\n",
    "    english_stopwords = set([\n",
    "        \"i\", \"me\", \"my\", \"myself\", \"we\", \"our\", \"ours\", \"ourselves\", \"you\", \"your\", \n",
    "        \"yours\", \"yourself\", \"yourselves\", \"he\", \"him\", \"his\", \"himself\", \"she\", \n",
    "        \"her\", \"hers\", \"herself\", \"it\", \"its\", \"itself\", \"they\", \"them\", \"their\", \n",
    "        \"theirs\", \"themselves\", \"what\", \"which\", \"who\", \"whom\", \"this\", \"that\", \n",
    "        \"these\", \"those\", \"am\", \"is\", \"are\", \"was\", \"were\", \"be\", \"been\", \"being\", \n",
    "        \"have\", \"has\", \"had\", \"having\", \"do\", \"does\", \"did\", \"doing\", \"a\", \"an\", \n",
    "        \"the\", \"and\", \"but\", \"if\", \"or\", \"because\", \"as\", \"until\", \"while\", \"of\", \n",
    "        \"at\", \"by\", \"for\", \"with\", \"about\", \"against\", \"between\", \"into\", \"through\", \n",
    "        \"during\", \"before\", \"after\", \"above\", \"below\", \"to\", \"from\", \"up\", \"down\", \n",
    "        \"in\", \"out\", \"on\", \"off\", \"over\", \"under\", \"again\", \"further\", \"then\", \"once\"\n",
    "    ])\n",
    "    \n",
    "    tokens = [token for token in tokens if token not in english_stopwords]\n",
    "    \n",
    "    # Join tokens back into a string\n",
    "    processed_text = \" \".join(tokens)\n",
    "    \n",
    "    return processed_text\n",
    "\n",
    "def replace_digits_with_words(text):\n",
    "    # Map digits to their word representations (0-9 for simplicity)\n",
    "    digit_map = {\n",
    "        '0': 'zero', '1': 'one', '2': 'two', '3': 'three', '4': 'four',\n",
    "        '5': 'five', '6': 'six', '7': 'seven', '8': 'eight', '9': 'nine'\n",
    "    }\n",
    "    \n",
    "    # Replace digits with their word representations\n",
    "    for digit, word in digit_map.items():\n",
    "        text = text.replace(digit, word)\n",
    "    \n",
    "    return text\n",
    "\n",
    "def remove_special_characters(text):\n",
    "    # Define a set of special characters to remove\n",
    "    special_characters = set(\"!\\\"#$%&'()*+,-./:;<=>?@[\\\\]^_`{|}~\")\n",
    "    \n",
    "    # Remove special characters from the text\n",
    "    cleaned_text = \"\".join(char for char in text if char not in special_characters)\n",
    "    \n",
    "    return cleaned_text\n",
    "def to_lower_case(text):\n",
    "    lower_text = \"\"\n",
    "    for char in text:\n",
    "        # Check if character is uppercase\n",
    "        if 'A' <= char <= 'Z':\n",
    "            # Convert uppercase character to lowercase\n",
    "            lower_text += chr(ord(char) + 32)\n",
    "        else:\n",
    "            lower_text += char\n",
    "    return lower_text\n",
    "def remove_urls(text):\n",
    "    # Split text into words\n",
    "    words = text.split()\n",
    "    # Filter out words that do not start with 'http' or 'https'\n",
    "    filtered_words = [word for word in words if not (word.startswith('http://') or word.startswith('https://'))]\n",
    "    # Join the filtered words back into a string\n",
    "    return ' '.join(filtered_words)\n",
    "\n",
    "\n",
    "# Load your DataFrame from 'training.tsv' assuming it contains text data\n",
    "df = pd.read_csv('training.tsv', sep='\\t')\n",
    "\n",
    "# Apply the preprocess function to each element in the DataFrame\n",
    "df['cleaned_text'] = df['text'].apply(preprocess)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
